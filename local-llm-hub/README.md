# Local LLM Hub

This project helps you set up and manage multiple uncensored local Large Language Models (LLMs) on your computer, allowing you to interact with them through a unified interface.

## Project Overview

The Local LLM Hub provides:
- Automated installation of popular open-source LLM models
- A unified interface to interact with all installed models
- Tools to compare responses across different models
- Options to customize model settings and parameters

## Supported Models

This project supports installing and running the following uncensored LLMs locally:

1. **Llama 3.1 (8B/70B)** - Latest Meta AI's open-source model
2. **Mistral (7B)** - Efficient open-source model with strong performance
3. **Vicuna (7B/13B)** - Tuned from Llama with conversational capabilities
4. **WizardLM** - Optimized for instruction following
5. **Orca 2** - Research model with strong reasoning capabilities
6. **Phi-3** - Microsoft's small but capable model
7. **MPT** - MosaicML's permissively licensed models
8. **Falcon** - Technology Innovation Institute's powerful model
9. **Pythia** - EleutherAI's suite of language models
10. **RWKV** - RNN-based alternative to transformer architecture

## Requirements

- Windows, macOS, or Linux
- Python 3.8+
- 16GB+ RAM (32GB+ recommended for larger models)
- GPU with 8GB+ VRAM for better performance
- 100GB+ free disk space for storing multiple models

## Getting Started

1. Install the required dependencies
2. Run the model installer script
3. Launch the LLM interface
4. Connect to your preferred model

Detailed instructions are in the following sections.

## Installation

See the [INSTALLATION.md](INSTALLATION.md) file for detailed setup instructions.

## Usage

See the [USAGE.md](USAGE.md) file for information on how to use the interface.

## License

This project is released under the MIT License.
